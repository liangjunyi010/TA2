import os
import pickle
import pandas as pd
from tqdm import tqdm
import torch
from torch.utils.data import Dataset
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from system_prompt import *
from args import *

if not os.path.exists(f"./Output/Clean"):
    os.makedirs(f"./Output/Clean")


class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def gen(tokenizer, model, system_prompt, prompt, max_new_tokens):
    full_prompt = "<<SYS>>\n" + system_prompt + "\n<</SYS>>\n\n" + prompt.strip()
    if args.model == "vicuna":
        full_prompt = system_prompt + prompt.strip()
    if args.verbose:
        print(f"\033[95mInput: {full_prompt}\033[0m")
    all_tokens = tokenizer.encode(f"[INST] {full_prompt.strip()} [/INST]")
    tokens = torch.tensor(all_tokens).unsqueeze(0).to("cuda")

    end = torch.tensor(tokenizer.encode("[/INST]")[1:]).to("cuda")
    size_tokens, size_end = tokens[0].size(0), end.size(0)
    instruction_position = -1
    if size_end > size_tokens:
        instruction_position = len(end) - 2
    for i in range(size_tokens - size_end + 1):
        if torch.equal(tokens[i : i + size_end], end):
            instruction_position = i + len(end) - 1

    for layer in model.model.layers:
        layer.after_position = instruction_position
    generated = model.generate(inputs=tokens, max_new_tokens=max_new_tokens, top_k=1)
    return tokenizer.batch_decode(generated)[0]


if __name__ == "__main__":
    print(f"\033[92mProcessing {args.dataset} using {args.model}... ...\033[0m")

    if args.model == "llama":
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
        model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-chat-hf",
            device_map="auto",
            trust_remote_code=True,
            do_sample=True,
            top_k=1,
        )
    if args.model == "vicuna":
        tokenizer = AutoTokenizer.from_pretrained("lmsys/vicuna-7b-v1.5")
        model = AutoModelForCausalLM.from_pretrained(
            "lmsys/vicuna-7b-v1.5",
            device_map="auto",
            trust_remote_code=True,
            do_sample=True,
            top_k=1,
        )

    if args.dataset == "tqa":
        if args.model == "llama":
            system_prompt = system_prompt_llama

        if args.model == "vicuna":
            if args.prompt_type == "freeform":
                system_prompt = system_prompt_vicuna
            if args.prompt_type == "choice":
                system_prompt = system_prompt_vicuna_choice_tqa

        if args.prompt_type == "freeform":
            with open("./Dataset/TruthfulQA/tqa_prompt.pkl", "rb") as f:
                data = pickle.load(f)

        if args.prompt_type == "choice":
            with open(f"./Dataset/TruthfulQA/tqa_A_B.json", "r") as f:
                data = pd.read_json(f)
                data = data["question"]

        result = []
        for instruction in tqdm(data, total=len(data), desc="Processing prompts"):
            out = gen(
                tokenizer,
                model,
                system_prompt,
                instruction,
                max_new_tokens=args.max_token,
            )
            res = out.split("[/INST]")[1]
            if args.verbose:
                print(f"\033[92mResponse: {res}\033[0m")
            result.append(res)

        with open(
            f"./Output/Clean/tqa_clean_{args.model}_{args.prompt_type}.pkl", "wb"
        ) as f:
            pickle.dump(result, f)

    if args.dataset == "toxigen":
        if args.model == "llama":
            system_prompt = system_prompt_llama

        if args.model == "vicuna":
            system_prompt = system_prompt_vicuna

        if args.prompt_type == "freeform":
            with open("./Dataset/ToxiGen/new_toxigen_prompt.pkl", "rb") as f:
                data = pickle.load(f)

        if args.prompt_type == "choice":
            with open(f"./Dataset/ToxiGen/toxigen_A_B.json", "r") as f:
                data = pd.read_json(f)
                data = data["question"]

        result = []
        for instruction in tqdm(data, total=len(data), desc="Processing prompts"):
            out = gen(
                tokenizer,
                model,
                system_prompt,
                instruction,
                max_new_tokens=args.max_token,
            )
            res = out.split("[/INST]")[1]
            if args.verbose:
                print(f"\033[92mOut: {res}\033[0m")
            result.append(res)

        with open(
            f"./Output/Clean/toxigen_clean_{args.model}_{args.prompt_type}.pkl",
            "wb",
        ) as f:
            pickle.dump(result, f)

    if args.dataset == "bold":
        if args.model == "llama":
            system_prompt = system_prompt_llama

        if args.model == "vicuna":
            if args.prompt_type == "freeform":
                system_prompt = system_prompt_vicuna
            if args.prompt_type == "choice":
                system_prompt = system_prompt_vicuna_choice_bold

        if args.prompt_type == "freeform":
            with open("./Dataset/BOLD/new_bold_prompt.pkl", "rb") as f:
                data = pickle.load(f)

        if args.prompt_type == "choice":
            with open(f"./Dataset/BOLD/bold_A_B.json", "r") as f:
                data = pd.read_json(f)
                data = data["question"]

        result = []
        for instruction in tqdm(data, total=len(data), desc="Processing prompts"):
            out = gen(
                tokenizer,
                model,
                system_prompt,
                instruction,
                max_new_tokens=args.max_token,
            )
            res = out.split("[/INST]")[1]
            if args.verbose:
                print(f"\033[92mResponse: {res}\033[0m")
            result.append(res)

        with open(
            f"./Output/Clean/bold_clean_{args.model}_{args.prompt_type}.pkl", "wb"
        ) as f:
            pickle.dump(result, f)

    if args.dataset == "harmful":
        if args.model == "llama":
            system_prompt = system_prompt_llama

        if args.model == "vicuna":
            system_prompt = system_prompt_vicuna

        if args.prompt_type == "freeform":
            with open("./Dataset/Harmful/harmful_prompt.pkl", "rb") as f:
                data = pickle.load(f)

        if args.prompt_type == "choice":
            with open(f"./Dataset/Harmful/harmful_A_B.json", "r") as f:
                data = pd.read_json(f)
                data = data["question"]

        result = []
        for instruction in tqdm(data, total=len(data), desc="Processing prompts"):
            out = gen(
                tokenizer,
                model,
                system_prompt,
                instruction,
                max_new_tokens=args.max_token,
            )
            res = out.split("[/INST]")[1]
            if args.verbose:
                print(f"\033[92mResponse: {res}\033[0m")
            result.append(res)

        with open(
            f"./Output/Clean/harmful_clean_{args.model}_{args.prompt_type}.pkl",
            "wb",
        ) as f:
            pickle.dump(result, f)

    print(f"\033[92mFinished processing {args.dataset} using {args.model}!\033[0m")
